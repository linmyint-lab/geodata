{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linmyint-lab/geodata/blob/main/ParticalDetctionWS_CMU2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Studying Vertical Total Electron Content (TEC) and Geomagnetic Field**\n",
        "\n",
        "This program generates a one-day Vertical Total Electron Content (TEC) maps and diurnal plot using data from Coordinated Data Analysis Web (CDAWeb) available at https://cdaweb.gsfc.nasa.gov/ and plots Geomagnetic fields using INTERMAGNET data availab at https://intermagnet.org/\n",
        "\n",
        "Please note that this program is intended for academic purposes only. It was developed by Lin Myint (linminmin.my@kmitl.ac.th) and team members of THAI GNSS and SPACE WEATHER - KMITL.\n",
        "\n",
        "**Coordinated Data Analysis Web (CDAWeb)**\n",
        "CDAWeb contains selected public non-solar heliophysics data from current and past heliophysics missions and projects. Many datasets from current missions are updated regularly (even daily), including reprocessing older time periods, and SPDF only preserves the latest version. To find all of the public data and documents archived by the SPDF, see the SPDF archive. To search for additional heliophysics data products, check the heliophysics data portal.\n",
        "\n",
        "**International Real-time Magnetic Observatory Network**\n",
        "Welcome to INTERMAGNET - the global network of observatories, monitoring the Earth's magnetic field. At this site you can find data and information from geomagnetic observatories around the world\n",
        "\n"
      ],
      "metadata": {
        "id": "dsgrDyuWHPKa"
      },
      "id": "dsgrDyuWHPKa"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pandas"
      ],
      "metadata": {
        "id": "Q78tEZqDgVlX"
      },
      "id": "Q78tEZqDgVlX",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d2a603e8-07bd-4e36-95f7-78b9b30be67c",
      "metadata": {
        "id": "d2a603e8-07bd-4e36-95f7-78b9b30be67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7472afc7-ffad-4b42-e31e-a391a211ee1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pycdfpp Module is installed.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import sys, time, os, subprocess, re\n",
        "try: # python module for processing NASA cdf data\n",
        "  import pycdfpp\n",
        "  print(\"pycdfpp Module is installed.\")\n",
        "except ImportError:\n",
        "  print(\"pycdfpp Module is not installed os installing\")\n",
        "  subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pycdfpp'])\n",
        "finally:\n",
        "  import pycdfpp\n",
        "import numpy as np\n",
        "import glob # tothe pathnames management\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import geopandas as gpd # for processing geodata\n",
        "import pandas as pd # for data processing\n",
        "from urllib import request\n",
        "from urllib.error import URLError\n",
        "from contextlib import ExitStack\n",
        "import shutil\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from shapely.geometry import Point, box\n",
        "import matplotlib.dates as mdates\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I_obRTdpjU1"
      },
      "source": [
        "## Importing the library files and reading the dataset file"
      ],
      "id": "1I_obRTdpjU1"
    },
    {
      "cell_type": "code",
      "source": [
        "# https://imag-data.bgs.ac.uk/GIN_V1/GINForms2\n",
        "#\n",
        "#\n",
        "#----------------------------------------------------------------------\n",
        "# upd_op_co:         Increment and serialize the operation count\n",
        "# Input parameters:  op_count the current operation number\n",
        "# Returns:           op_count is incremented and returned (and written to disk)\n",
        "def upd_op_co(op_count):\n",
        "  op_count = op_count + 1\n",
        "  with open('counter.dat', 'w') as f:\n",
        "    f.write('%d' % op_count)\n",
        "  return op_count\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# safemd:            Safely create a folder (no error if it already exists)\n",
        "# Input parameters:  folder the directory to create\n",
        "#                    op_number the operation number for this call\n",
        "#                    op_count the current operation number\n",
        "# Returns:           op_count is incremented and returned (and written to disk)\n",
        "def safemd (folder, op_number, op_count):\n",
        "  if op_number >= op_count:\n",
        "    if op_number == 0:\n",
        "      print ('Creating directories...')\n",
        "    try:\n",
        "      os.makedirs (folder, exist_ok=True)\n",
        "    except OSError:\n",
        "      print ('Error: unable to create directory: ' + str(folder))\n",
        "      sys.exit (1)\n",
        "    op_count = upd_op_co (op_count)\n",
        "  return op_count\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# getfile:           Download a file from a web server\n",
        "# Input parameters:  url URL to download from\n",
        "#                    local_file local file to download to\n",
        "#                    n_retries number of retries to do\n",
        "#                    op_number the operation number for this call\n",
        "#                    gin_username the username of the GIN (or empty string)\n",
        "#                    gin_password the username of the GIN (or empty string)\n",
        "#                    proxy_address address of proxy server (or empty string)\n",
        "#                    n_folders the number of folders to create\n",
        "#                    n_downloads the number of files to download\n",
        "#                    op_count the current operation number\n",
        "# Returns:           op_count is incremented and returned (and written to disk)\n",
        "def getfile (url, local_file, n_retries, op_number, gin_username,\n",
        "             gin_password, proxy_address, n_folders,\n",
        "             n_downloads, op_count):\n",
        "  if op_number >= op_count:\n",
        "    # tell the user what's going on\n",
        "    percent = ((op_number - n_folders) * 100) / n_downloads\n",
        "    print ('%d%% - downloading file: %s' % (percent, local_file))\n",
        "\n",
        "    # remove any existing file\n",
        "    try:\n",
        "      os.remove (local_file)\n",
        "    except FileNotFoundError:\n",
        "      pass\n",
        "    except OSError:\n",
        "      print ('Error: unable to remove file: ' + str(local_file))\n",
        "      sys.exit (1)\n",
        "\n",
        "    # handle authentication and proxy server\n",
        "    proxy = auth = None\n",
        "    if len (proxy_address) > 0:\n",
        "      proxy = req.ProxyHandler({'http': proxy_address, 'https': proxy_address})\n",
        "    if len (gin_username) > 0:\n",
        "      auth = req.HTTPBasicAuthHandler()\n",
        "      auth.add_password (realm=None,\n",
        "                         uri='https://imag-data.bgs.ac.uk/GIN_V1',\n",
        "                         user=gin_username,\n",
        "                         passwd=gin_password)\n",
        "    if url.startswith ('https'):\n",
        "      default_handler = request.HTTPSHandler\n",
        "    else:\n",
        "      default_handler = request.HTTPHandler\n",
        "    if auth and proxy:\n",
        "      opener = request.build_opener(proxy, auth, default_handler)\n",
        "    elif auth:\n",
        "      opener = request.build_opener(auth, default_handler)\n",
        "    elif auth:\n",
        "      opener = request.build_opener(proxy, default_handler)\n",
        "    else:\n",
        "      opener = request.build_opener(default_handler)\n",
        "\n",
        "    # download the file\n",
        "    success = False\n",
        "    while (not success) and (n_retries > 0):\n",
        "      try:\n",
        "        with opener.open (url) as f_in:\n",
        "          with open (local_file, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out, 4096)\n",
        "        success = True\n",
        "      except (URLError, IOError, OSError):\n",
        "        n_retries -= 1\n",
        "    if not success:\n",
        "      print ('Error: cannot download ' + local_file)\n",
        "      sys.exit (1)\n",
        "\n",
        "    # rename IAGA-2002 files\n",
        "    dt = None\n",
        "    try:\n",
        "      with open(local_file, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "          if re.search ('^ Data Type', line):\n",
        "            dt = line[24:25].lower()\n",
        "    except (IOError, OSError):\n",
        "      pass\n",
        "    if dt:\n",
        "      if not dt.isalpha():\n",
        "        dt = None\n",
        "    if dt:\n",
        "      new_local_file = local_file[:len(local_file) - 7] + dt + local_file[len(local_file) - 7:]\n",
        "      try:\n",
        "        os.remove (new_local_file)\n",
        "      except (FileNotFoundError, OSError):\n",
        "        pass\n",
        "      try:\n",
        "        os.rename (local_file, new_local_file)\n",
        "      except (IOError, OSError):\n",
        "        print ('Warning: unable to rename ' + local_file + ' to ' + new_local_file)\n",
        "    else:\n",
        "      print ('Warning: unable to determine data type for renaming of ' + local_file)\n",
        "\n",
        "    op_count = upd_op_co (op_count)\n",
        "  return op_count, new_local_file\n"
      ],
      "metadata": {
        "id": "hItjXE-spco9"
      },
      "execution_count": 14,
      "outputs": [],
      "id": "hItjXE-spco9"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enter a date of interest\n",
        "date_study = \"2024-02-05\" # @param {type:\"date\"}\n",
        "date_study = pd.Timestamp(date_study);\n",
        "# creating a cdf file name from  CDAWeb\n",
        "cdf_file = \"gps_tec15min_igs_\"+date_study.strftime(\"%Y%m%d\")+\"_v01.cdf\""
      ],
      "metadata": {
        "id": "9deNCiNvzo2j"
      },
      "id": "9deNCiNvzo2j",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "223b9440-8b37-4093-b693-da8c132f263d",
      "metadata": {
        "id": "223b9440-8b37-4093-b693-da8c132f263d"
      },
      "source": [
        "Reading TEC data from NASA Coordinated Data Analysis Web (CDAWeb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(cdf_file): # if data cdf file has been downloaded before\n",
        "  print(\"That cdf file exist\")\n",
        "else:# if not, the data file will be downloaded\n",
        "  url_cdaweb = \"https://cdaweb.gsfc.nasa.gov/pub/data/gps/tec15min_igs/\"\n",
        "  url_cdf =os.path.join(url_cdaweb,str(date_study.year),cdf_file )\n",
        "  query_parameters = {\"downloadformat\": \"cdf\"}\n",
        "  try:\n",
        "      response = requests.get(url_cdf, params=query_parameters)\n",
        "      if response.status_code == 200:\n",
        "          with open(cdf_file, \"wb\") as f:\n",
        "              f.write(response.content)\n",
        "      else:\n",
        "          print(\"Failed to download the file:\", response.status_code)\n",
        "  except requests.ConnectionError as e:\n",
        "      # Handle the connection error\n",
        "      print(\"Connection Error:\", e)\n",
        "      print(\"The program will now exit.\")\n",
        "      sys.exit(1)  # Exit with a non-zero status code to indicate\n",
        "\n",
        "cdf_data = pycdfpp.load(cdf_file) # reading cdf file\n",
        "print(cdf_data)"
      ],
      "metadata": {
        "id": "IJDXK2M-4A0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a5ab26-a81b-4d70-a51f-7612a8ba2f0c"
      },
      "id": "IJDXK2M-4A0M",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That cdf file exist\n",
            "CDF:\n",
            "  version: 3.9.0\n",
            "  majority: row\n",
            "  compression: None\n",
            "\n",
            "Attributes:\n",
            "  TITLE: \"GPS total electron content 15minute maps\"\n",
            "  Project: \"GPS>Global Positioning System\"\n",
            "  Source_name: \"GPS>Global Positioning System\"\n",
            "  Discipline: \"Space Physics>Magnetospheric  Ionospheric Interactions\"\n",
            "  Descriptor: \"tec15min> Total Electron Content 15minute\"\n",
            "  Data_type: \"IGS>Intern. GPS Service\"\n",
            "  Data_version: \"1\"\n",
            "  TEXT: [ [ \"The IGS global system of satellite tracking stations, Data Centers, and Analysis Centers puts high-quality GPS data and data products on line in near real time to meet the objectives of a wide range of scientific and engineering applications and studies.  The IGS collects, archives, and distributes GPS observation data sets of sufficient accuracy to satisfy the objectives of a wide range of applications and experimentation.  These data sets are used by the IGS to generate the data products mentioned above which are made available to interested users through the Internet.  In particular, the accuracies of IGS products are sufficient for the improvement and extension of the International Terrestrial Reference Frame (ITRF), the monitoring of solid Earth deformations, the monitoring of Earth rotation and variations in the liquid Earth (sea level, ice-sheets, etc.), for scientific satellite orbit determinations, ionosphere monitoring, and recovery of precipitable water vapor measurements.  \", \"The primary mission of the International GPS Service, as stated in the organization's 2002-2007 Strategic Plan, is \", \"    The International GPS Service is committed to providing the highest quality data and products as the standard for global navigation satellite systems (GNSS) in support of Earth science research, multidisciplinary applications, and education. These activities aim to advance scientific understanding of the Earth system components and their interactions, as well as to facilitate other applications benefiting society.\", \"The IGS Terms of Reference (comparable to the by-laws of the organization) describes in broad terms the goals and organization of the IGS.  To accomplish its mission, the IGS has a number of components: an international network of over 350 continuously operating dual-frequency GPS stations, more than a dozen regional and operational data centers, three global data centers, seven analysis centers and a number of associate or regional analysis centers. The Central Bureau for the service is located at the Jet Propulsion Laboratory, which maintains the Central Bureau Information System (CBIS) and ensures access to IGS products and information. An international Governing Board oversees all aspects of the IGS.  The IGS is an approved service of the International Association of Geodesy since 1994 and is recognized as a member of the Federation of Astronomical and Geophysical Data Analysis Services (FAGS) since 1996. \", \"The IGS collects, archives, and distributes GPS observation data sets of sufficient accuracy to meet the objectives of a wide range of scientific and engineering applications and studies. These data sets are used to generate the following products:\", \"  * GPS satellite ephemerides\", \"  * GLONASS satellite ephemerides\", \"  * Earth rotation parameters\", \"  * IGS tracking station coordinates and velocities\", \"  * GPS satellite and IGS tracking station clock information\", \"  * Zenith tropospheric path delay estimates\", \"  * Global ionospheric maps\", \"IGS products support scientific activities such as improving and extending the International Earth Rotation Service (IERS) Terrestrial Reference Frame (ITRF), monitoring deformations of the solid Earth and variations in the liquid Earth (sea level, ice sheets, etc.), and in Earth rotation, determining orbits of scientific satellites and monitoring the ionosphere. For example, geodynamics investigators who use GPS in local regions can include data from one or more nearby IGS stations, fix the site coordinates from such stations to their ITRF values, and more importantly, use the precise IGS orbits without further refinement. Data from an investigator's local network can then be analyzed with maximum accuracy and minimum computational burden. Furthermore, the results will be in a well-defined global reference frame.  An additional aspect of IGS products is for the densification of the ITRF at a more regional level. This is accomplished through the rigorous combination of regional or local network solutions utilizing the Solution Independent Exchange Format (SINEX) and a process defined in the densification section.  In the future, the IGS infrastructure could become a valuable asset for support of new ground-based applications -- and could also contribute to space-based missions in which highly accurate flight and ground differential techniques are required.\" ] ]\n",
            "  ADID_ref: \" \"\n",
            "  Logical_source: \"gps_tec15min_igs\"\n",
            "  Logical_file_id: \"gps_tec15min_igs_00000000_v01\"\n",
            "  Logical_source_description: \"GPS-deduced 15-minute Total Electron Content (TEC) global maps and movies, UPC= U Politec. Catalonia Barcelona Spain\"\n",
            "  PI_name: \"International Global Navigation Satellite Systems (GNSS) Service Iono Working Group\"\n",
            "  PI_affiliation: \" \"\n",
            "  NSSDC_id: \" \"\n",
            "  Mission_group: [ [ \"GPS\", \"!___Magnetospheric Data\", \"!___ITM Data including Earth Imaging and Ground-Based\" ] ]\n",
            "  Instrument_type: \"Particles (space)\"\n",
            "  LINK_TEXT: [ [ \"CDDIS ftp archive: \", \"JPL-GPS-TEC: \", \"Movies of popular \", \"Description and comparison of the TEC maps \" ] ]\n",
            "  LINK_TITLE: [ [ \"IGS IONO files in IONEX format\", \"Real-time TEC maps for the whole globe\", \"geomagnetic storms\", \"from different analysis centers\" ] ]\n",
            "  HTTP_LINK: [ [ \"https://cddis.nasa.gov/Data_and_Derived_Products/GNSS/atmospheric_products.html#iono\", \"https://iono.jpl.nasa.gov/latest_rti_global.html\", \"https://spdf.gsfc.nasa.gov/pub/data/gps/movies/\", \"https://spdf.gsfc.nasa.gov/pub/data/gps/00README.txt\" ] ]\n",
            "  cdawlib_idl_row_notranspose: \"FALSE\"\n",
            "  spase_DatasetResourceID: \"spase://NASA/NumericalData/IGS/GPS_Receiver/TEC/PT15M\"\n",
            "\n",
            "Variables:\n",
            "  Epoch: [ 96 ], [CDF_EPOCH], record vary:True, compression: None\n",
            "  tecUQR: [ 96, 71, 73 ], [CDF_REAL4], record vary:True, compression: GNU GZIP\n",
            "  rmsUQR: [ 96, 71, 73 ], [CDF_REAL4], record vary:True, compression: GNU GZIP\n",
            "  UQRnumStations: [ 96 ], [CDF_INT2], record vary:True, compression: GNU GZIP\n",
            "  UQRnumSatellites: [ 96 ], [CDF_INT2], record vary:True, compression: GNU GZIP\n",
            "  lat: [ 1, 71 ], [CDF_REAL4], record vary:False, compression: None\n",
            "  lon: [ 1, 73 ], [CDF_REAL4], record vary:False, compression: None\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1acdead-5358-4da1-bbe3-8568487d0b67",
      "metadata": {
        "id": "e1acdead-5358-4da1-bbe3-8568487d0b67"
      },
      "source": [
        "Reading World map dataset from https://www.naturalearthdata.com/\n",
        "and Dip Equator line dataset from a private Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8fbb241c-40c6-4f61-883f-17a69c7d26b7",
      "metadata": {
        "id": "8fbb241c-40c6-4f61-883f-17a69c7d26b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32bdc5e-8d70-4c64-8e5f-d5ae23bf51e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world map file is downloading\n",
            "That dip Equator file exist\n"
          ]
        }
      ],
      "source": [
        "zip_file = \"ne_110m_admin_0_countries.zip\" # world map data zip file\n",
        "zip_file_url ='https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip'\n",
        "zip_file_url = \"https://github.com/linmyint-lab/geodata/raw/main/ne_110m_admin_0_countries.zip\"\n",
        "\n",
        "if os.path.isfile(zip_file): #if the zip file has been downloaded before\n",
        "  print(\"The world map file is exist\")\n",
        "else: # if not, the file is downloading\n",
        "  print(\"The world map file is downloading\")\n",
        "\n",
        "  query_parameters = {\"downloadformat\": \"zip\"}\n",
        "  try:\n",
        "    response = requests.get(zip_file_url,headers=headers, params=query_parameters)\n",
        "    if response.status_code == 200:\n",
        "      # Save the file or process the response\n",
        "      with open(zip_file, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    else:\n",
        "      print(\"Failed to download the file:\", response.status_code)\n",
        "      sys.exit(1)  # Exit with a non-zero status code to indicate an error\n",
        "  except requests.ConnectionError as e:\n",
        "    # Handle the connection error\n",
        "    print(\"Connection Error:\", e)\n",
        "    print(\"The program will now exit.\")\n",
        "    sys.exit(1)  # Exit with a non-zero status code to indicate an error\n",
        "# reading world map dataset as geodataframe\n",
        "world = gpd.read_file(zip_file)\n",
        "\n",
        "dipEqLin_file = \"dipEquatorLine.csv\"\n",
        "if os.path.isfile(dipEqLin_file): # if data cdf file has been downloaded before\n",
        "  print(\"That dip Equator file exist\")\n",
        "else:\n",
        "  # reading dip equator line coordinates as pandas dataframe and geodataframe\n",
        "  eq_path = \"https://raw.githubusercontent.com/linmyint-lab/geodata/1688cd5d607ed2aea13edaf8aede81f7682903af/dipEquatorLine.csv\"\n",
        "  print(\"The world map file is downloading\")\n",
        "  try:\n",
        "    response = requests.get(eq_path, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "      # Save the file or process the response\n",
        "      with open(dipEqLin_file, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    else:\n",
        "      print(\"Failed to download the file:\", response.status_code)\n",
        "  except requests.ConnectionError as e:\n",
        "    # Handle the connection error\n",
        "    print(\"Connection Error:\", e)\n",
        "    print(\"The program will now exit.\")\n",
        "    sys.exit(1)  # Exit with a non-zero status code to indicate an error\n",
        "\n",
        "# reading equator line and make it as geodataframe\n",
        "df_eq = pd.read_csv(dipEqLin_file ,header=None,names=['lon','lat'])\n",
        "gdf_eq = gpd.GeoSeries(gpd.points_from_xy(df_eq['lon'], df_eq['lat']),crs = \"EPSG:4326\") # reading world map dataset as geodataframe\n",
        "del df_eq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad15099e-db2b-49ce-bb4e-21e07f141ca5",
      "metadata": {
        "id": "ad15099e-db2b-49ce-bb4e-21e07f141ca5"
      },
      "source": [
        "Checking information of cdf file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9a24800-a389-493c-a7b6-e7b68353f044",
      "metadata": {
        "id": "b9a24800-a389-493c-a7b6-e7b68353f044"
      },
      "source": [
        "Reading data as as geodataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e75433f-d32a-4a16-ab06-d187dbf34997",
      "metadata": {
        "id": "1e75433f-d32a-4a16-ab06-d187dbf34997"
      },
      "outputs": [],
      "source": [
        "Epoch = pycdfpp.to_datetime64(cdf_data['Epoch'])\n",
        "pd_Epoch = pd.to_datetime(Epoch)\n",
        "lat = cdf_data['lat'].values.flatten()\n",
        "lon = cdf_data['lon'].values.flatten()\n",
        "num_stat = cdf_data['UQRnumStations'].values\n",
        "num_sat = cdf_data['UQRnumSatellites'].values\n",
        "tecUQR = cdf_data['tecUQR'].values\n",
        "x, y = np.meshgrid(lon,lat)\n",
        "iterables = [['TEC'],pd_Epoch.strftime('%b %d %Y %H:%M:%S')]\n",
        "# col_list = pd.MultiIndex.from_product(iterables, names=[\"TEC\", \"Epoch\"])\n",
        "# data_df = pd.DataFrame(columns =col_list)\n",
        "data_df = pd.DataFrame()\n",
        "for i in range(pd_Epoch.shape[0]):\n",
        "    #data_df['TEC',pd_Epoch[i].strftime('%b %d %Y %H:%M:%S')] = tecUQR[i].flatten()\n",
        "    data_df[pd_Epoch[i].strftime('%b %d %Y %H:%M:%S')] = tecUQR[i].flatten()\n",
        "data_df['lon']= x.flatten();\n",
        "data_df['lat']= y.flatten();\n",
        "# Create a GeoDataFrame by specifying the DataFrame and geometry column\n",
        "data_gdf = gpd.GeoDataFrame(data_df, geometry=gpd.points_from_xy(data_df['lon'], data_df['lat']),crs = \"EPSG:4326\")\n",
        "del data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating VTEC Global Map"
      ],
      "metadata": {
        "id": "Lf1L7T9bADaK"
      },
      "id": "Lf1L7T9bADaK"
    },
    {
      "cell_type": "code",
      "source": [
        "my_cmap = plt.get_cmap('coolwarm',)\n",
        "fig, ax = plt.subplots(figsize=(10.0, 6.0));\n",
        "world.plot(ax = ax, color='lightgrey', edgecolor='black')\n",
        "data_gdf.plot(column = data_gdf.columns[0], ax = ax, cmap = my_cmap, vmin=0, vmax=100, alpha = 0.6, markersize=35,\n",
        "              legend= True, legend_kwds = {\"label\":\"TEC (TECu)\", \"orientation\": \"horizontal\"} )\n",
        "gdf_eq.plot(ax = ax, markersize=1, edgecolor='black')\n",
        "ax.set_xlim(-180,180)\n",
        "ax.set_ylim(-90,90)\n",
        "ax.set_title('Vertical Total Electron Content (TEC) Map at ' + pd_Epoch[0].strftime('%b %d %Y %H:%M:%S'))\n",
        "plt.close()\n",
        "def animate_plot(i):\n",
        "  ax.clear()\n",
        "  world.plot(ax = ax, color='lightgrey', edgecolor='black')\n",
        "  data_gdf.plot(column = data_gdf.columns[i], ax = ax, cmap = my_cmap, vmin=0, vmax=100, alpha = 0.6, markersize=35)\n",
        "  gdf_eq.plot(ax = ax, markersize=1, edgecolor='black')\n",
        "  ax.set_xlim(x.min(),x.max())\n",
        "  ax.set_ylim(y.min(),y.max())\n",
        "  ax.set_title('Vertical Total Electron Content (TEC) Map at ' + pd_Epoch[i].strftime('%b %d %Y %H:%M:%S')+ ' UTC')\n",
        "  return fig\n",
        "anim = animation.FuncAnimation(fig,animate_plot,len(pd_Epoch),interval=100, repeat=False)\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "d--Jsr_AclfE"
      },
      "id": "d--Jsr_AclfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating VTEC Global Map near magnetic equator"
      ],
      "metadata": {
        "id": "ZcERJCqXABUD"
      },
      "id": "ZcERJCqXABUD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895b8bb2-68f2-49af-8302-58fc4f18e3df",
      "metadata": {
        "id": "895b8bb2-68f2-49af-8302-58fc4f18e3df"
      },
      "outputs": [],
      "source": [
        "# polygon = box(-180, -30, 180, 30)\n",
        "# poly_gdf = gpd.GeoDataFrame([1], geometry=[polygon], crs = \"EPSG:4326\")\n",
        "# world_clipped = world.clip(polygon)\n",
        "# my_cmap = plt.get_cmap('coolwarm')\n",
        "# fig, ax = plt.subplots(figsize=(12,5));\n",
        "# world.clip(polygon).plot(ax = ax, color='lightgrey', edgecolor='black')\n",
        "# poly_gdf.clip(polygon).boundary.plot(ax=ax, color=\"red\")\n",
        "# data_gdf.clip(polygon).plot(column = data_gdf.columns[0], ax = ax, cmap = my_cmap, vmin=0, vmax=100,\n",
        "#                             alpha = 0.6, markersize=50, legend= True,\n",
        "#                             legend_kwds = {\"label\":\"TEC (TECu)\", \"orientation\": \"horizontal\"})\n",
        "# gdf_eq.clip(polygon).plot(ax = ax, markersize=1, edgecolor='black')\n",
        "# ax.set_xlim(polygon.bounds[0],polygon.bounds[2])\n",
        "# ax.set_ylim(polygon.bounds[1],polygon.bounds[3])\n",
        "# ax.set_title('Vertical Total Electron Content (TEC) Map at ' + pd_Epoch[0].strftime('%b %d %Y %H:%M:%S'))\n",
        "# #plt.gca().set_aspect(4)\n",
        "# plt.close()\n",
        "# def animate_plot(i):\n",
        "#     ax.clear()\n",
        "#     world.clip(polygon).plot(ax = ax, color='lightgrey', edgecolor='black')\n",
        "#     data_gdf.clip(polygon).plot(column = data_gdf.columns[i], ax = ax, cmap = my_cmap,\n",
        "#                                 vmin=0, vmax=100, alpha = 0.6, markersize=100)\n",
        "#     gdf_eq.clip(polygon).plot(ax = ax, markersize=1, edgecolor='black')\n",
        "#     ax.set_xlim(polygon.bounds[0],polygon.bounds[2])\n",
        "#     ax.set_ylim(polygon.bounds[1],polygon.bounds[3])\n",
        "#     ax.set_title('Vertical Total Electron Content (TEC) Map at ' + pd_Epoch[i].strftime('%b %d %Y %H:%M:%S')+ ' UTC')\n",
        "#     return fig\n",
        "# anim = animation.FuncAnimation(fig,animate_plot,len(pd_Epoch),interval=300, repeat=False)\n",
        "# HTML(anim.to_html5_video())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ploting for each country"
      ],
      "metadata": {
        "id": "J6ClYOdT-AFu"
      },
      "id": "J6ClYOdT-AFu"
    },
    {
      "cell_type": "code",
      "source": [
        "# list of countries\n",
        "list_c=world['SOVEREIGNT'].values.copy()\n",
        "list_c.sort()\n",
        "print(list_c)"
      ],
      "metadata": {
        "id": "2HlJoaF54cb4"
      },
      "id": "2HlJoaF54cb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## choose a country that you want to study its ionospheric TEC\n",
        "\n",
        "Change from Thailand to Vietnam"
      ],
      "metadata": {
        "id": "QcjSBFwbnNd5"
      },
      "id": "QcjSBFwbnNd5"
    },
    {
      "cell_type": "code",
      "source": [
        "country_map= world[world['SOVEREIGNT']=='Thailand']\n",
        "c_b = country_map.total_bounds\n",
        "my_cmap = plt.get_cmap('coolwarm')\n",
        "fig, ax = plt.subplots(figsize=(12,6));\n",
        "country_map.plot(ax = ax, color='lightgrey', edgecolor='black')\n",
        "data_gdf.clip(box(c_b[0],c_b[1],c_b[2],c_b[3])).plot(column = data_gdf.columns[0], ax = ax, cmap = my_cmap,\n",
        "                            vmin=0, vmax=100, alpha = 0.8, markersize=1500, legend= True,\n",
        "                            legend_kwds = {\"label\":\"TEC (TECu)\", \"orientation\": \"horizontal\"})\n",
        "ax.set_title('Vertical Total Electron Content (TEC) Map at ' + pd_Epoch[0].strftime('%b %d %Y %H:%M:%S'))\n",
        "ax.set_xlim(c_b[0],c_b[2])\n",
        "ax.set_ylim(c_b[1],c_b[3])\n",
        "plt.close()\n",
        "def animate_plot(i):\n",
        "    ax.clear()\n",
        "    country_map.plot(ax = ax, color='lightgrey',edgecolor='black')\n",
        "    data_gdf.clip(box(c_b[0],c_b[1],c_b[2],c_b[3])).plot(column = data_gdf.columns[i], ax = ax, cmap = my_cmap,\n",
        "                                 vmin=0, vmax=100,alpha = 0.8, markersize=1500)\n",
        "    ax.set_xlim(c_b[0],c_b[2])\n",
        "    ax.set_ylim(c_b[1],c_b[3])\n",
        "    ax.set_title('Vertical Total Electron Content (TEC) Map at ' + pd_Epoch[i].strftime('%b %d %Y %H:%M:%S')+ ' UTC')\n",
        "    return fig\n",
        "anim = animation.FuncAnimation(fig,animate_plot,len(pd_Epoch),interval=300, repeat=False)\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "ZJCfcGPKUmee"
      },
      "id": "ZJCfcGPKUmee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting diurnal vTEC data of each coordinate location"
      ],
      "metadata": {
        "id": "1kATC8VdYOnk"
      },
      "id": "1kATC8VdYOnk"
    },
    {
      "cell_type": "code",
      "source": [
        "r = country_map.total_bounds\n",
        "cor_list = data_gdf.clip(country_map).get_coordinates(ignore_index=True)\n",
        "cor_list.sort_values(by=['x','y'],inplace=True)\n",
        "cor_list.reset_index(inplace=True, drop = True)\n",
        "cor_list"
      ],
      "metadata": {
        "id": "-UEN-6eUergB"
      },
      "id": "-UEN-6eUergB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose one index number of coordinates in the list above\n",
        "idx = 0 # @param {type:\"integer\"}\n",
        "\n",
        "time_idx=pycdfpp.to_datetime64(cdf_data[\"Epoch\"])\n",
        "fig,ax =plt.subplots( figsize=(12,5))\n",
        "lon_index =np.where(lon==cor_list['x'][idx])[0]\n",
        "lat_index =np.where(lat==cor_list['y'][idx])[0]\n",
        "ax.plot(time_idx, cdf_data[\"tecUQR\"].values[:,lat_index,lon_index], marker = 'o')\n",
        "ax.set_xlim(time_idx.min(),time_idx.max())\n",
        "ax.grid()\n",
        "ax.set( xlabel='Time UTC', ylabel='TEC (TECu)', title= 'Vertical TEC ')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45)\n",
        "ax.xaxis.set_major_locator(mdates.HourLocator(interval=1))  # Set grid line interval to 1 hour\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aEVkUeUmtfWV"
      },
      "id": "aEVkUeUmtfWV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting for all coordinates\n",
        "time_idx=pycdfpp.to_datetime64(cdf_data[\"Epoch\"]) + pd.Timedelta(7,\"h\")\n",
        "try:\n",
        "  fig,ax =plt.subplots(nrows =len(cor_list), ncols =1, figsize=(8,12), sharex=True)\n",
        "  for ind in range(len(cor_list)):\n",
        "    lon_index =np.where(lon==cor_list['x'][ind])[0]\n",
        "    lat_index =np.where(lat==cor_list['y'][ind])[0]\n",
        "    ax[ind].plot(time_idx, cdf_data[\"tecUQR\"].values[:,lat_index,lon_index])\n",
        "    ax[ind].set( ylabel='TEC (TECu)', title= 'Vertical TEC at lon: ' +  str(lon[lon_index][0]) + ' and  lat:' + str(lat[lat_index][0]))\n",
        "    ax[ind].set_xlim(time_idx.min(),time_idx.max())\n",
        "    ax[ind].grid()\n",
        "    ax[ind].xaxis.set_major_locator(mdates.HourLocator(interval=1))  # Set grid line interval to 1 hour\n",
        "\n",
        "  ax[ind].set( xlabel='Time in UTC', ylabel='TEC (TECu)', title= 'Vertical TEC at lon: ' +  str(lon[lon_index][0]) + ' and  lat:' + str(lat[lat_index][0]))\n",
        "  plt.setp(ax[ind].get_xticklabels(), rotation=45)\n",
        "  plt.show()\n",
        "except:\n",
        "  print(\"Error\")\n",
        "  plt.close()\n",
        "# xlabel='Time in UTC',"
      ],
      "metadata": {
        "id": "8MPRmjOWnyiL"
      },
      "id": "8MPRmjOWnyiL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_idx=pycdfpp.to_datetime64(cdf_data[\"Epoch\"]) + pd.Timedelta(7,\"h\")\n",
        "fig,ax =plt.subplots( figsize=(12,5))\n",
        "label = []\n",
        "time_idx= pd.to_datetime(time_idx)\n",
        "# for i in range(time_idx)\n",
        "for ind in range(len(cor_list)):\n",
        "  lon_index =np.where(lon==cor_list['x'][ind])[0]\n",
        "  lat_index =np.where(lat==cor_list['y'][ind])[0]\n",
        "  ax.plot(time_idx, cdf_data[\"tecUQR\"].values[:,lat_index,lon_index], marker = 'o')\n",
        "  label.append(str(lon[lon_index][0]) + ' and ' + str(lat[lat_index][0]))\n",
        "\n",
        "ax.set_xlim(time_idx.min(),time_idx.max())\n",
        "ax.grid()\n",
        "ax.set( xlabel='Time in UT', ylabel='TEC (TECu)', title= 'Vertical TEC ')\n",
        "ax.legend(label)\n",
        "ax.xaxis.set_major_locator(mdates.HourLocator(interval=1))  # Set grid line interval to 1 hour\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# xlabel='Time in UTC',"
      ],
      "metadata": {
        "id": "xE_vrm8z_xlX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xE_vrm8z_xlX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejmeqj-BBqHP"
      },
      "source": [
        "### Enter the date and the station's IAGA code\n",
        "Click the INTERMANET link https://intermagnet.org/metadata/#/imos to look the list of stations.\n",
        "\n",
        "Check the data available at https://intermagnet.org/new_data_download.html\n",
        "\n",
        "Enter the interested date and the station's IAGA code below"
      ],
      "id": "ejmeqj-BBqHP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bedy1kzEBqfs"
      },
      "source": [
        "#@title Enter Year, Month and Station Name to be studied { vertical-output: true }\n",
        "\n",
        "# date_input = '2022-01-01' #@param {type:\"date\"}\n",
        "stat_name = 'DLT' #@param {type:\"string\"}\n",
        "date_input = date_study\n",
        "\n",
        "\n",
        "date_input = pd.Timestamp(date_input)\n",
        "total_days = 1"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Bedy1kzEBqfs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoUbtkVq3xCB"
      },
      "source": [
        "## Data Reading and Preprocessing\n",
        "\n",
        "### Read magnetometer dataset as Dataframe and Screening\n",
        "\n",
        "Read the dataset files of the day interested for calculation.\n",
        "\n",
        "The dataset files will be downloaded to a folder name Magnetometerdata at local space.\n",
        "\n",
        "### Download the data files"
      ],
      "id": "eoUbtkVq3xCB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSat_NGlMW4i"
      },
      "source": [
        "#### Read the data as dataframes. Dalat"
      ],
      "id": "FSat_NGlMW4i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqgR3k-zpHQb"
      },
      "source": [
        "# Downloading the data\n",
        "#!/usr/bin/env python3\n",
        "# download.py: Download geomagnetic data from Edinburgh GIN\n",
        "#\n",
        "# This script is designed to download geomagnetic data\n",
        "# from the Edinburgh INTERMAGNET Geomagnetic Information Node.\n",
        "# To use the script, follow these steps:\n",
        "#\n",
        "# 1.) Create a new directory.\n",
        "# 2.) Move to the new directory and copy the script there.\n",
        "# 3.) Execute the script by typing 'python3 download.py'.\n",
        "#\n",
        "# The script keeps track of its progress through the list of files\n",
        "# to download. If it fails at any time, you can restart it and it will\n",
        "# resume at the point where it failed.\n",
        "#\n",
        "#\n",
        "# Configurable parameters - these variables control use of proxy\n",
        "# servers at the users site - there's also an option to use\n",
        "# authentication - change them as required (though the\n",
        "# defaults should work OK)\n",
        "gin_username = ''\n",
        "gin_password = ''\n",
        "proxy_address = ''\n",
        "n_retries = 4\n",
        "date_input = pd.Timestamp(date_input) # change to Timestamp from string\n",
        "year_month = date_input.strftime('%Y %B %d')\n",
        "date_temp = date_input\n",
        "time_start = date_input # the start time (minute) of target date/month\n",
        "time_end = date_input - pd.to_timedelta(1,'minute') # the last time (minute) of traget date/month\n",
        "loc_folder =  os.path.join(os.getcwd(),'Magnetometerdata', stat_name+ date_input.strftime('%Y%m')) # folder to save\n",
        "stat_name_l = stat_name.lower() # name of station\n",
        "Geo_Lo, K9_limit = 0, 0 #\n",
        "\n",
        "\n",
        "try:\n",
        "  with open ('counter.dat') as f: # download the data\n",
        "    op_count = int(f.read())\n",
        "    print('Information: resuming download after previous failure')\n",
        "except (IOError, ValueError, OSError):\n",
        "  op_count = 0\n",
        "n_folders = 1\n",
        "n_downloads = 1\n",
        "\n",
        "folder = loc_folder\n",
        "op_count = safemd(folder, 0, op_count)\n",
        "loc_file = os.path.join(loc_folder,'m'+ date_input.strftime('%Y%m%d.') + stat_name_l)\n",
        "ser_link = 'https://imag-data.bgs.ac.uk/GIN_V1/GINServices?Request=GetData&format=IAGA2002&testObsys=0&observatoryIagaCode='+stat_name+'&samplesPerDay=1440&orientation=Native&publicationState=adj-or-rep&recordTermination=UNIX&dataStartDate='+date_temp.strftime('%Y-%m-%d')+'&dataDuration=1'\n",
        "#ser_link = 'https://imag-data.bgs.ac.uk/GIN_V1/GINServices?Request=GetData&format=IAGA2002&testObsys=0&observatoryIagaCode='+stat_name+'&samplesPerDay=1440&orientation=Native&publicationState=definitive&recordTermination=UNIX&dataStartDate='+date_temp.strftime('%Y-%m-%d')+'&dataDuration=1'\n",
        "op_count, new_loc_file = getfile (ser_link, loc_file, n_retries, 1, gin_username, gin_password, proxy_address, n_folders, n_downloads, op_count)\n",
        "  #'https://imag-data.bgs.ac.uk/GIN_V1/GINServices?Request=GetData&format=IAGA2002&testObsys=0&observatoryIagaCode=DLT&samplesPerDay=1440&orientation=Native&publicationState=adj-or-rep&recordTermination=UNIX&dataStartDate=2023-11-08&dataDuration=1'\n",
        "  # tidy up\n",
        "print ('100% - data download complete')\n",
        "os.remove ('counter.dat')\n",
        "\n",
        "with open(new_loc_file, \"rb\") as fr:\n",
        "  for j in range(30):\n",
        "    line = fr.readline()\n",
        "    if b'K9-limit' in line:\n",
        "      K9_limit  = line.decode('utf-8').split()[2]\n",
        "      K9_limit = int(K9_limit) # Change K9 limit according to the information\n",
        "      #print(f'K9 lower limit is {K9_limit}')\n",
        "    if b'Geodetic Longitude' in line:\n",
        "      Geo_Lo = line.decode('utf-8').split()[2]\n",
        "      Geo_Lo = float(Geo_Lo)\n",
        "    if b'DATE' in line:\n",
        "      line_no = j\n",
        "      print('there is DATE')\n",
        "    #print(line) # display header information of file\n",
        "  fr.seek(0)\n",
        "print('finished')\n",
        "print(f'K9 lower limit is {K9_limit}')\n",
        "\n",
        "  # Calculate the local time difference from longitude\n",
        "if Geo_Lo < 180:\n",
        "  LT_diff = Geo_Lo/15 # for East\n",
        "else:\n",
        "  LT_diff = (180 - Geo_Lo)/15 # for West"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "YqgR3k-zpHQb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTl2INMBDE_Q"
      },
      "source": [
        "# Create the list of files stored at local drive and check\n",
        "temp = date_input.strftime('%Y%m%d')\n",
        "temp = temp[0:5]+'*'+temp[5:-1]\n",
        "target_file =os.path.join(loc_folder, 'm'+temp +'*.'+ stat_name_l) # change to filename)\n",
        "csv_files = glob.glob(target_file) # read all file name\n",
        "if len(csv_files) == total_days :\n",
        "  print(r'Selected ' + date_input.strftime('%Y %b') )\n",
        "  print(f'The number of days with data is {len(csv_files)} including one day.')\n",
        "elif len(csv_files) in range(1, total_days-2) :\n",
        "  print(r'There are some days without data')\n",
        "  print(f'The number of days with data is {len(csv_files)}')\n",
        "else:\n",
        "  print(r'There is no data for such time period')\n",
        "\n",
        "# read each file name from list and read them dataframe.\n",
        "if len(csv_files) > 0:\n",
        "  csv_files.sort() # sort the file as date\n",
        "    # column names (need to rewrite read from file)\n",
        "  col_names =['YYYY-MM-DD_UT','UT', 'DOYUT', 'X (nT)','Y (nT)', 'Z (nT)','G']\n",
        "    # Read all file, convert them to Dataframe and add to a list of Dataframe\n",
        "  list_dfs = [pd.read_csv(f_name, delim_whitespace=True,  skiprows=line_no+1, index_col=None,\n",
        "                          names=col_names, encoding= 'unicode_escape') for f_name in csv_files] # encoding= 'unicode_escape'\n",
        "  Mag_df = pd.DataFrame() # A dataframe to keep all Magnetic Date\n",
        "  for i in range(len(list_dfs)): # temp_df in enumerate(list_dfs):\n",
        "      # Convert the date time format to YYYY-MM-DD HH:MM:SSS using to_datetime and to_timedelta (difference in time)\n",
        "    date_temp = date_input + pd.Timedelta(i,'day')\n",
        "    list_dfs[i]['YYYY-MM-DD_UT'] = pd.to_datetime(list_dfs[i]['YYYY-MM-DD_UT'])+pd.to_timedelta(list_dfs[i]['UT']).dt.round(\"s\")\n",
        "    list_dfs[i].rename(columns={\"YYYY-MM-DD_UT\":\"DateTime_UT\"},errors='raise',inplace=True) # Change the column name\n",
        "    list_dfs[i].set_index('DateTime_UT',inplace=True) # set as index column\n",
        "    if list_dfs[i].index.duplicated().any(): # delete any dublicated number\n",
        "      print(\"There is duplicated data to be dropped.\")\n",
        "      list_dfs[i].drop_duplicates(keep=\"first\",inplace=True)\n",
        "    dt = (list_dfs[i].index[-1]- list_dfs[i].index[0])\n",
        "    if list_dfs[i].shape[0] != dt.total_seconds()/60 +1:\n",
        "     list_dfs[i] = list_dfs[i].resample('T',base=0).mean() # make every 1 minutes if there is data gap, add nan\n",
        "     list_dfs[i].interpolate(method='linear',limit_direction='forward',inplace=True)\n",
        "     print(f\"Data gap is replace by interpolation method for day {i+1}\")\n",
        "    if dt.total_seconds()/60 +1 != 24 * 60:\n",
        "     print(f\"Data is not complete in day {i+1}\")\n",
        "    # to add NAN data frame if the data is missing at begining and end\n",
        "    if list_dfs[i].index[0] != date_temp:\n",
        "      time_gap = list_dfs[i].index[0] - date_temp\n",
        "      temp_idx = pd.date_range(start=date_temp, end=date_temp + time_gap - pd.to_timedelta(1,'minute'), freq='T')\n",
        "      temp_df = pd.DataFrame(columns=list_dfs[i].columns,index=temp_idx,dtype= float)\n",
        "      list_dfs[i] = pd.concat([temp_df, list_dfs[i]])\n",
        "      print(f'There is no data at the beginning {date_temp}')\n",
        "      del temp_df, temp_idx, time_gap\n",
        "    day_end = date_temp + pd.to_timedelta(1,'day') - pd.to_timedelta(1,'minute')\n",
        "    if list_dfs[i].index[-1] != day_end:\n",
        "      time_gap = day_end - list_dfs[i].index[-1]\n",
        "      temp_idx = pd.date_range(start=list_dfs[i].index[-1] + pd.to_timedelta(1,'minute'), end = day_end, freq='T')\n",
        "      temp_df = pd.DataFrame(columns=list_dfs[i].columns,index=temp_idx,dtype= float)\n",
        "      list_dfs[i] = pd.concat([list_dfs[i], temp_df])\n",
        "      print(f'There is no data at the end {date_temp}')\n",
        "      del temp_df, temp_idx, time_gap\n",
        "\n",
        "\n",
        "else: print(\"Sorry, There is no file.\")\n",
        "\n",
        "# Removing the outliner more than 3 * delta\n",
        "mag_col = ['X (nT)','Y (nT)', 'Z (nT)','G']\n",
        "temp_df = pd.DataFrame()\n",
        "for i in range(len(list_dfs)):\n",
        "  list_dfs[i] = list_dfs[i].replace(99999.00,np.nan)\n",
        "  list_dfs[i] = list_dfs[i].replace(-9999.00,np.nan)\n",
        "  temp_df = list_dfs[i][mag_col].copy()\n",
        "  mean_t = temp_df.mean(numeric_only=True)\n",
        "  std_t = temp_df.std(numeric_only=True)\n",
        "  z_score = np.abs(temp_df - mean_t)/std_t\n",
        "  temp_df = temp_df[z_score <= 3.5]\n",
        "  list_dfs[i][mag_col] = temp_df\n",
        "del temp_df"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "sTl2INMBDE_Q"
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(nrows=3, ncols=1, figsize=(20.0,9.0), sharex=True) #figsize=(20.0,8.0)\n",
        "temp_df = list_dfs[0]\n",
        "ax[0].plot(temp_df.index,temp_df['X (nT)'],linewidth=1)\n",
        "ax[1].plot(temp_df.index,temp_df['Y (nT)'],linewidth=1)\n",
        "ax[2].plot(temp_df.index,temp_df['Z (nT)'],linewidth=1)\n",
        "\n",
        "  # Mag_median_df = Mag_median_df.resample('D').last()\n",
        "ax[0].set( ylabel='X (nT)', title= 'X component from ' +  year_month + ' at ' + stat_name)\n",
        "ax[1].set( ylabel='Y (nT)', title= 'Y component from ' +  year_month + ' at ' + stat_name)\n",
        "ax[2].set(xlabel='Date in UTC', ylabel='Z (nT)', title= 'Z component from ' +  year_month+ ' at ' + stat_name)\n",
        "\n",
        "ax[0].set_xlim(list_dfs[0].index.min(), list_dfs[0].index.max())\n",
        "ax[1].set_xlim(list_dfs[0].index.min(), list_dfs[0].index.max())\n",
        "ax[2].set_xlim(list_dfs[0].index.min(), list_dfs[0].index.max())\n",
        "# ax[0].xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "# ax[2].xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "plt.setp(ax[1].get_xticklabels(), rotation=45)\n",
        "ax[0].grid()\n",
        "ax[1].grid()\n",
        "ax[2].grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LhwA8zt-PJv_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LhwA8zt-PJv_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf-CEvaYDdzl"
      },
      "source": [
        "#### Plot before substracting median\n",
        "\n",
        "\n"
      ],
      "id": "Jf-CEvaYDdzl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6S7skljDdzt"
      },
      "source": [
        "mag_col = ['X (nT)','Y (nT)', 'Z (nT)','G']\n",
        "Mag_df = pd.DataFrame() # A dataframe to keep all Magnetic Date\n",
        "for i in range(len(list_dfs)):\n",
        "  temp_df = list_dfs[i]\n",
        "  temp_df.loc[:,mag_col] = temp_df.loc[:,mag_col] -temp_df.loc[:,mag_col].median()\n",
        "  Mag_df = pd.concat([Mag_df, temp_df])\n",
        "\n",
        "\n",
        "# After removing the daily median\n",
        "fig,ax = plt.subplots(nrows=3, ncols=1, figsize=(20.0,8.0), sharex=True) #figsize=(20.0,8.0)\n",
        "\n",
        "ax[0].plot(temp_df.index,temp_df['X (nT)'],linewidth=1)\n",
        "ax[1].plot(temp_df.index,temp_df['Y (nT)'],linewidth=1)\n",
        "ax[2].plot(temp_df.index,temp_df['Z (nT)'],linewidth=1)\n",
        "\n",
        "  # Mag_median_df = Mag_median_df.resample('D').last()\n",
        "ax[0].set( ylabel='X (nT)', title= 'X component from ' +  year_month + ' at ' + stat_name)\n",
        "ax[1].set( ylabel='Y (nT)', title= 'Y component from ' +  year_month + ' at ' + stat_name)\n",
        "ax[2].set(xlabel='Date in UTC', ylabel='Z (nT)', title= 'Z component from ' +  year_month+ ' at ' + stat_name)\n",
        "\n",
        "ax[0].set_xlim(list_dfs[0].index.min(), list_dfs[0].index.max())\n",
        "ax[1].set_xlim(list_dfs[0].index.min(), list_dfs[0].index.max())\n",
        "ax[2].set_xlim(list_dfs[0].index.min(), list_dfs[0].index.max())\n",
        "# ax[0].xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "# ax[2].xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "plt.setp(ax[1].get_xticklabels(), rotation=45)\n",
        "ax[0].grid()\n",
        "ax[1].grid()\n",
        "ax[2].grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "S6S7skljDdzt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing correlation between two variables over time"
      ],
      "metadata": {
        "id": "4ZEexRRiH-R3"
      },
      "id": "4ZEexRRiH-R3"
    },
    {
      "cell_type": "code",
      "source": [
        "val_1 = cdf_data[\"tecUQR\"].values[:,lat_index,lon_index].flatten()\n",
        "val_2 = temp_df['X (nT)'].values[np.arange(0,1440,15)].flatten()\n",
        "val_df = pd.DataFrame({'TEC':val_1, 'Magnetic': val_2})"
      ],
      "metadata": {
        "id": "pi6la7LMIPWp"
      },
      "id": "pi6la7LMIPWp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(nrows=2, ncols=1, figsize=(20.0,9.0), sharex=True) #figsize=(20.0,8.0)\n",
        "ax[0].plot(time_idx,val_df ['TEC'],linewidth=1)\n",
        "ax[1].plot(time_idx, val_df ['Magnetic'],linewidth=1)\n",
        "\n",
        "  # Mag_median_df = Mag_median_df.resample('D').last()\n",
        "ax[0].set(ylabel='TEC', title= 'TEC from ' +  year_month)\n",
        "ax[1].set(xlabel='Date in UTC', ylabel='X (nT)', title= 'X component from ' +  year_month+ ' at ' + stat_name)\n",
        "\n",
        "ax[0].set_xlim(time_idx.min(), time_idx.max())\n",
        "ax[1].set_xlim(time_idx.min(), time_idx.max())\n",
        "\n",
        "plt.setp(ax[1].get_xticklabels(), rotation=45)\n",
        "ax[0].grid()\n",
        "ax[1].grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nEhjf3afZDvv"
      },
      "id": "nEhjf3afZDvv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "corr, _ = pearsonr(val_df['TEC'],val_df['Magnetic'])\n",
        "sns.set(style='white', font_scale=1.2)\n",
        "xlim = val_df['TEC'].max()+1\n",
        "ylim = val_df['Magnetic'].max() + 1\n",
        "g = sns.JointGrid(data=val_df, x = 'TEC', y='Magnetic', height=5)\n",
        "g.plot_joint(sns.scatterplot, s=100, alpha=.5)\n",
        "g = g.plot_joint(sns.regplot, color=\"xkcd:muted blue\")\n",
        "#g.plot(sns.regplot, sns.boxplot)\n",
        "g = g.plot_marginals(sns.histplot, kde=False, bins=12, color=\"xkcd:bluey grey\")\n",
        "g.ax_joint.text(1, 9, f'r = {round(corr,2)}', fontstyle='italic')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "u329_uk_Kev5"
      },
      "id": "u329_uk_Kev5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "*   Find two days: magnetic quiet day and disturbed day using the following link\n",
        "https://www.spaceweatherlive.com/en/archive.html\n",
        "\n",
        "*   Obverse variations of ionospheric TEC and round geomagnetic field using this program\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tRZUBkjaFeS_"
      },
      "id": "tRZUBkjaFeS_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}